---
title: "Biomarkers of ASD"
subtitle: "Group 8"
author: "Candis Wu, Jade Michael O'Brien, Joseph Zaki, Daniel Ariel Ledvin"
date: last-modified
published-title: "Updated"
editor: visual
format: html
code-copy: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

```{r}
# load any other packages and read data here
library(tidyverse)
library(infer)
library(randomForest)
library(tidymodels)
library(modelr)
library(yardstick)
library(glmnet)
library(ggplot2)
load('../data/biomarker-clean.RData')
raw_data <- read_csv('../data/biomarker-raw.csv')
```

## Abstract

This report explores the identification of biomarkers associated with Autism Spectrum Disorder (ASD) using data from L. Hewitson's experiment in 2021. Though the original report has since been retracted due to unreliable methodology, this report explores the data and tests alternate methods to those used in the original study. We started with testing why the data was log-transformed, concluding the log-transformation simply made the data more closely resemble a normal distribution, making it easier to work with. We then tested to see if there are any proteins that seem to commonly occur as outliers and if there are any subjects that seem to consistently have outlier protein values. After this analysis, it was apparent that none of the proteins occur disproportionately as outliers, however there were a few patients with over 40 outlier proteins, most of which were in the typically developing (TD) group.

To explore alternative predictive biomarkers, we performed feature selection using a training partition to avoid overfitting and replicated the selection across different methods. Additionally, we experimented with combining sets of top predictive proteins through both a hard intersection and a fuzzy intersection approach, finding that the latter introduced more flexibility while retaining strong classifiers. Performance metrics demonstrated that these panels could contain other potential reliable biomarkers for ASD diagnosis and reinforced the strength of the DERM and IgD proteins as biomarkers.

## Dataset

Data was collected on 154 male pediatric subjects, 76 of which were diagnosed with autism spectrum disorder (ASD) and the other 78 were determined to be typically developing (TD). Demographic data was collected for all participants, including age, ethnicity, co-morbidities, and psychiatric medications. The mean age of the ASD group was 5.6 years old, and the mean age of subjects from the TD group was 5.7 years old. Additionally, the SOMAScan platform 1.3k was used to measure the levels of 1,317 proteins in each subject. The levels of these proteins were then log10-transformed, standardized, and then outliers were trimmed to values of -3 or 3 depending on their sign.

## Summary of published analysis

<!-- Summarize the methodology of the paper in 1-3 paragraphs. You need not explain the methods in depth as we did in class; just indicate what methods were used and how they were combined. If possible, include a diagram that depicts the methodological design. (Quarto has support for GraphViz and Mermaid flowcharts.) Provide key results: the proteins selected for the classifier and the estimated accuracy. -->

## Findings

### Impact of preprocessing and outliers (Task 1 & 2)

#### Log-Transform

```{r}
raw_data_long <- raw_data |> 
  select(Group:`Induced myeloid leukemia cell differentiation protein Mcl-1`, -`Target Full Name`) |> 
  mutate(ID = row_number()) |> 
  pivot_longer(`E3 ubiquitin-protein ligase CHIP`:`Induced myeloid leukemia cell differentiation protein Mcl-1`) |> 
  filter(ID > 1) |> 
  mutate(value = as.numeric(value))

raw_data_long |> 
  filter(value <= 20000) |> 
  ggplot(aes(x = value)) + 
  geom_histogram(bins = 30) + 
  facet_wrap(~name) +
  labs(title = "Original Distributions of Protein Levels", y = "Count", x = 'Protein Level')

raw_data_long |> 
  group_by(name) |> 
  summarize("Mean Level" = mean(na.omit(value))) |> 
  kableExtra::kable(
    caption = "Mean Levels of Selected Proteins",
    col.names = c("Protein", "Mean Level") 
  ) |> 
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    font_size = 14
  )
```

We see that the protein levels in `biomarker-raw.csv` seem to be incredibly high, with a lot of the proteins having mean levels at or above 1000. Additionally, we see hat a lot of the distributions are pretty heavily right-skewed, and thus are likely not to be normally distributed.

In contrast, when we look at the same proteins with log-transformed levels below, we see that the now the average protein level is close to 3, which is significantly lower than it was. Also, we see that the log-transform has decreased the skew of the distribution of the protein levels, causing them to be more similar to a normal distribution. We want to work with normally distributed data as often as possible, as it is easy to work with, and many data analytic techniques require normally distributed data.

```{r}
raw_data_long_log <- raw_data_long |> 
  mutate(log_value = log10(value))

raw_data_long_log |> 
  ggplot(aes(x = log_value)) + 
  geom_histogram(bins = 30) + 
  facet_wrap(~name) +
  labs(title = "Distributions of Protein Levels After Log-Transform", y = "Count", x = 'Log Protein Level')

raw_data_long_log |> 
  group_by(name) |> 
  summarize("Mean Log Level" = mean(na.omit(log_value))) |> 
  kableExtra::kable(
    caption = "Mean Log Levels of Selected Proteins",
    col.names = c("Protein", "Mean Log Level") 
  ) |> 
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    font_size = 14
  )

```

#### Examining Outliers

```{r}
# get names
var_names <- read_csv(here::here('data/biomarker-raw.csv'), 
                     col_names = F, 
                     n_max = 2, 
                     col_select = -(1:2)) %>%
  t() %>%
  as_tibble() %>%
  rename(name = V1, 
         abbreviation = V2) %>%
  na.omit()

# function for trimming outliers (good idea??)
trim <- function(x, .at){
  x[abs(x) > .at] <- sign(x[abs(x) > .at])*.at
  return(x)
}

# read in data
biomarker_clean_outliers <- read_csv(here::here('data/biomarker-raw.csv'), 
         skip = 2,
         col_select = -2L,
         col_names = c('group', 
                       'empty',
                       pull(var_names, abbreviation),
                       'ados'),
         na = c('-', '')) %>%
  filter(!is.na(group)) %>%
  # log transform, center and scale, and trim
  mutate(across(.cols = -c(group, ados), 
                ~ (scale(log10(.x))[, 1]))) %>%
  # reorder columns
  select(group, ados, everything())

```

#### 

```{r}
longer_outliers <- biomarker_clean_outliers |> 
  mutate(ID = row_number()) |> 
  pivot_longer(CHIP:PLXB2)

longer_outliers |> 
  filter(value <= -3 | value >= 3) |> 
  group_by(name) |> 
  summarize(n = n()) |> 
  mutate(Group = case_when(
    n <= 2 ~ "0-2 subject with an outlier value",
    n > 2 & n <= 5 ~ "3-5 subjects with outlier values",
    TRUE ~ ">5 subjects with outlier values"
  )) |> 
  group_by(Group) |> 
  summarise('Number of Proteins' = n()) |> 
  kableExtra::kable(
    caption = "Grouping Proteins by Number of Subjects with Outlier Value",
    col.names = c("Protein Group", "Number of Proteins") 
  ) |> 
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    font_size = 14
  )
```

Most of the proteins had an outlier value in at least one participant, only `r 9 / 1317 * 100`% of proteins have 6 or more participants with an outlier value. The majority of proteins do not have have multiple subjects with an outlier value.

#### Are certain subjects more likely to have outlier values?

```{r}
outliers_per_id <- longer_outliers |> 
  filter(value <= -3 | value >= 3) |> 
  group_by(ID) |> 
  summarise('Number of Protiens with Outlier Values' = n(),
            group = group[1]) |> 
    mutate('Outlier Grouping' = case_when(
    `Number of Protiens with Outlier Values` <= 40 ~ "Less than or equal to 40 outlier proteins",
    `Number of Protiens with Outlier Values` > 40 & `Number of Protiens with Outlier Values` <= 100 ~ "40-100 outlier proteins",
    TRUE ~ " >100 outlier proteins"
  ))

outliers_per_id |> 
  group_by(`Outlier Grouping`) |> 
  summarize('Number of Subjects' = n()) |> 
  kableExtra::kable(
    caption = "Distribution of Amount of Outlier Proteins per Subjects",
    col.names = c("Outlier Grouping", "Number of Subjects") 
  ) |> 
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    font_size = 14
  )

outliers_per_id |> 
  filter(`Number of Protiens with Outlier Values` >= 40) |> 
  kableExtra::kable(
    caption = "Outlier Subjects",
    col.names = c("ID", "Number of Proteins with Outlier Values", "Group", "Outlier Grouping") 
  ) |> 
  kableExtra::kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = F,
    font_size = 14
  )
```

We see the majority of subjects seem to have less than or equal to 40 outlier proteins, with only 11 subjects having more than 40 proteins with outlier levels.

There do seem to be particular subjects that consistently have outlier protein values, with some subjects having outlier values in over 100 different proteins. Such subjects seem more likely to be in the typical developing group, with 8 subjects from the typical developing group having 40 or more proteins with outlier levels, and only 3 subjects from the ASD group having 40 or more proteins with outlier levels.

### Methodological variations (Task 3)

#### Part 1: Repeat the analysis

Note: carry out the entire selection procedure on a training partition -- in other words, set aside some testing data at the very beginning and don't use it until you are evaluating accuracy at the very end

```{r}
set.seed(101422)

biomarker_split <- biomarker_clean %>%
  initial_split(prop = 0.8)

train <- training(biomarker_split)
test <- testing(biomarker_split)
```

Replicating the feature selecting process on only the training dataset.

```{r}
# code taken from the inclass-analysis.R script
# getting the first panel of proteins using t-test
test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- train %>%
  select(-ados) %>%
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  nest(data = c(level, group)) %>% 
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  arrange(p_value) %>%
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)
```

```{r}
# getting the second panel of proteins using random forest
predictors <- train %>%
  select(-c(group, ados))

response <- train %>% pull(group) %>% factor()

set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

rf_out$confusion

proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)
```

```{r}
# getting the intercept of the two panels and fitting it into logistic regression

proteins_sstar <- intersect(proteins_s1, proteins_s2)

biomarker_sstar <- train %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

test <- test %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

fit <- glm(class ~ ., 
           data = biomarker_sstar, 
           family = 'binomial')

class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

test %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

We have these four proteins (DERM, IgD, TSP4, FSTL1) for our final panel.

#### Part 2: Choose a larger number of top predictive proteins

Note: more than ten and use each selection method

```{r}
# I am choosing 20 to allow a larger intercept values

############## MULTIPLE TESTING ###############

test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  select(-ados) %>%
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  nest(data = c(level, group)) %>% 
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  arrange(p_value) %>%
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

# select significant proteins
proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 20) %>% 
  pull(protein)

################## RANDOM FOREST ##################

# store predictors and response separately
predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 20) %>% 
  pull(protein)

#################### LOGISTIC REGRESSION ####################

proteins_sstar <- intersect(proteins_s1, proteins_s2)
proteins_sstar

biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(proteins_sstar)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)
```

Now we have ten proteins (DERM, RELT, MRC2, IgD, PTN, FSTL1, Cadherin-5, MMP-2, Notch 1, ALCAM) in our final panel.

```{r}
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

**Increasing the number of selected proteins**: By expanding from 10 to 20 top proteins, we've allowed for potentially more relevant predictors, increasing the specificity slightly but not affecting sensitivity much. This change leads to more comprehensive protein coverage, improving accuracy and ROC AUC.

#### Part 3: Fuzzy Intersection

Note: instead of a hard intersection use a fuzzy intersection to combine the sets of top predictive proteins across selection methods. How are results affected by each modification?

```{r}
################### MULTIPLE TESTING ###################

test_fn <- function(.df){
  t_test(.df, 
         formula = level ~ group,
         order = c('ASD', 'TD'),
         alternative = 'two-sided',
         var.equal = F)
}

ttests_out <- biomarker_clean %>%
  select(-ados) %>%
  pivot_longer(-group, 
               names_to = 'protein', 
               values_to = 'level') %>%
  nest(data = c(level, group)) %>% 
  mutate(ttest = map(data, test_fn)) %>%
  unnest(ttest) %>%
  arrange(p_value) %>%
  mutate(m = n(),
         hm = log(m) + 1/(2*m) - digamma(1),
         rank = row_number(),
         p.adj = m*hm*p_value/rank)

proteins_s1 <- ttests_out %>%
  slice_min(p.adj, n = 10) %>%
  pull(protein)

##################### RANDOM FOREST ####################

predictors <- biomarker_clean %>%
  select(-c(group, ados))

response <- biomarker_clean %>% pull(group) %>% factor()

set.seed(101422)
rf_out <- randomForest(x = predictors, 
                       y = response, 
                       ntree = 1000, 
                       importance = T)

proteins_s2 <- rf_out$importance %>% 
  as_tibble() %>%
  mutate(protein = rownames(rf_out$importance)) %>%
  slice_max(MeanDecreaseGini, n = 10) %>%
  pull(protein)

# Define the selection threshold for the fuzzy intersection (e.g., protein must appear in at least 2 out of 2 methods here)
fuzzy_threshold <- 1  # Adjust this based on the number of methods

# Combine selected proteins from different methods into a single data frame
protein_selection <- tibble(
  protein = unique(c(proteins_s1, proteins_s2)),
  t_test_selected = protein %in% proteins_s1,
  rf_selected = protein %in% proteins_s2
)

# Calculate an inclusion score based on the frequency of selection
# This fuzzy inclusion score can be adjusted as needed
protein_selection <- protein_selection %>%
  mutate(inclusion_score = rowSums(select(., t_test_selected, rf_selected)))

# Filter proteins based on the fuzzy threshold for inclusion
fuzzy_proteins <- protein_selection %>%
  filter(inclusion_score >= fuzzy_threshold) %>%
  pull(protein)

# Check results by comparing with hard intersection
hard_intersection_proteins <- intersect(proteins_s1, proteins_s2)

# Output for analysis
list(
  fuzzy_proteins = fuzzy_proteins,
  hard_intersection_proteins = hard_intersection_proteins
)
```

This approach allows for greater flexibility in protein selection allowing for more in the protein list, potentially including proteins with lesser predictive strength, while the hard method ensures higher confidence in selected biomarkers.

```{r}
biomarker_sstar <- biomarker_clean %>%
  select(group, any_of(fuzzy_proteins)) %>%
  mutate(class = (group == 'ASD')) %>%
  select(-group)

# partition into training and test set
set.seed(101422)
biomarker_split <- biomarker_sstar %>%
  initial_split(prop = 0.8)

# fit logistic regression model to training set
fit <- glm(class ~ ., 
           data = training(biomarker_split), 
           family = 'binomial')

# evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

testing(biomarker_split) %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
              truth = tr_c, pred,
              event_level = 'second')
```

**Using a fuzzy intersection**: This modification aims to retain proteins chosen by either selection method, rather than only the exact matches. Although it captures more potential biomarkers, it lowers sensitivity and accuracy slightly, indicating that the fuzzy selection introduces less specific, possibly weaker predictors. ROC AUC remains high but shows reduced confidence in positive classification, as shown by lower sensitivity.

### Improved classifier (Task 4)

#### Approach 1: LASSO Regression

```{r}
## Lasso Regression
###################

# Create class category
biomarker <- biomarker_clean %>% 
  select(-ados) %>% 
  mutate(class = as.numeric(group == 'ASD'))

# Partition
set.seed(102824)
partitions <- biomarker %>% 
  initial_split(prop=0.8)

x_train <- training(partitions) %>% 
  select(-group, -class) %>% 
  as.matrix()

y_train <- training(partitions) %>% 
  pull(class)

# Lambda Selection
cv_out <- cv.glmnet(x_train,
                    y_train,
                    family = 'binomial',
                    alpha = 1)
cv_out_df <- tidy(cv_out)

# Fit best lambda
best_lambda <- cv_out_df %>% 
  arrange(lambda) %>% 
  filter(nzero<7) %>% 
  slice(1) %>% 
  pull(lambda)

final_lasso <- glmnet(x_train,
                      y_train,
                      family = 'binomial',
                      alpha=1,
                      lambda = best_lambda)
lasso_coefs <- coef(final_lasso)
lasso_coefs_df <- as.data.frame(as.matrix(lasso_coefs)) %>% 
  filter(s0!=0)

proteins_lasso <- lasso_coefs_df %>% 
  mutate(proteins = rownames(lasso_coefs_df)) %>% 
  filter(rownames(lasso_coefs_df) != "intercept") %>%
  slice(-1) %>% 
  pull(proteins)

## Logistic Regression
#######################

# Pull proteins
fit_train <- training(partitions) %>% 
  select(group, any_of(proteins_lasso)) %>% 
  mutate(class = (group == 'ASD')) %>% 
  select(-group)
  
fit_test <- testing(partitions) %>% 
  select(group, any_of(proteins_lasso)) %>% 
  mutate(class = (group == 'ASD')) %>% 
  select(-group)

# Fit model with training data
fit <- glm(class ~ .,
           data = fit_train,
           family = 'binomial')

# Evaluate errors on test set
class_metrics <- metric_set(sensitivity, 
                            specificity, 
                            accuracy,
                            roc_auc)

fit_test %>%
  add_predictions(fit, type = 'response') %>%
  mutate(est = as.factor(pred > 0.5), tr_c = as.factor(class)) %>%
  class_metrics(estimate = est,
                truth = tr_c, pred,
                event_level = 'second') %>% knitr::kable()

```

Having previously been provided with a model that used a multiple t-test and random forest model for the purpose of variable selection, we attempted to find out if we can create another panel with similar, or better, results. In our first approach, we implemented the lasso regression model with the goal of shrinking all predictors to zero except for at most 6. Originally, we wanted to find less than ten, but with some trial and error, the results showed that using only 6 proteins produced the best results. We were left with PTN, MAPK2, Coagulation Factor IX, IgD, DERM, and aldolase A, which we plugged into a logistic model, then calculating the performance metrics using the test data. From the results, the roc_auc and sensitivity of our model were lower than those of the in-class analysis, however, the accuracy was the same and the specificity was higher.

#### Approach 2: Stepwise Variable Selection

```{r}
# partition
set.seed(101622)
partitions <- biomarker %>%
  initial_split(prop = 0.8, strata = class)

biomarker_train <- training(partitions)
biomarker_test <- testing(partitions)
```

In the second approach, we split the data into a train/test split at a 80/20 proportion stratified across `class` to ensure even amounts of ASD and TD boys in both sets. In this approach, we opted for a different method of selecting predictors. Here, we performed forward selection, starting with a null model containing only an intercept and iteratively adding each of the 18 proteins identified in the study:

![Possible Predictors Identified by Hewitson et al.](report_files/possiblePreds.PNG){fig-align="center"}

Performing Forward Stepwise Regression resulted in the below model:

```{r, message = T}
# All proteins as identified by the three methods in the paper
# First 5 are identified by all methods (core panel)
max_predictors <- c("DERM", "suPAR", "MAPK14", "IgD", "EPHB2", 
                    "ALCAM", "`eIF-4H`", "SOST", "C6", "Calcineurin",
                    "FCN1", "C1QR1", "PTN", "RELT", "`Prolactin Receptor`", "ROR1",
                    "GI24", "ARSB")

max_formula <- as.formula(paste("class ~", paste(max_predictors, collapse = "+")))

full_fit <- glm(max_formula, data = biomarker_train, family = binomial)
null_fit <- glm(class ~ 1, data = biomarker_train, family = binomial)

forward_model <- stats::step(null_fit, direction = "forward", 
                             scope = list(lower = null_fit, upper = full_fit), trace = 0)

summary(forward_model)

```

From the above output of `summary(forward_model)`, we see that 8 out of the 18 proteins were selected to be included as predictors of ASD. The selected proteins were **DERM, IgD, FCN1, eIF-4H, PTN, C6, EPHB2, and ROR1.**

Interestingly, the model found through forward stepwise selection did not include the proteins MAPK14 or suPAR, which is surprising considering that these proteins were part of the "core panel" found by Hewitson et al., meaning that each of the three methods used in the study identified these as being significant predictors of ASD. Despite the exclusion of these two "core" proteins, this model performed well on the testing data, achieving the below sensitivity, specificity, accuracy, and ROC AUC.

```{r, message = T}
class_metrics <- metric_set(sensitivity, specificity, accuracy, roc_auc)
model_predictors <- forward_model$model %>% select(-class) %>% colnames()

biomarker_test %>% 
  select(all_of(model_predictors), class) %>% 
  modelr::add_predictions(model = forward_model, type = "response") %>% 
  mutate(est = as.factor(pred>0.5), 
         tr = as.factor(ifelse(class == 1, T, F))) %>%
  class_metrics(truth = tr, estimate = est, pred, event_level = "second") %>% 
  knitr::kable()

```
